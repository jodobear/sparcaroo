{
  "providerProfiles": {
    "currentApiConfigName": "Gemini",
    "apiConfigs": {
      "Simpler": {
        "apiProvider": "deepseek",
        "apiModelId": "deepseek-chat",
        "apiKey": "",
        "openRouterApiKey": "",
        "openRouterModelId": "deepseek/deepseek-r1-distill-qwen-32b",
        "openRouterModelInfo": {
          "maxTokens": 8192,
          "contextWindow": 131072,
          "supportsImages": false,
          "supportsPromptCache": false,
          "inputPrice": 0.12,
          "outputPrice": 0.18,
          "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 72.6\n- MATH-500 pass@1: 94.3\n- CodeForces Rating: 1691\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
          "thinking": false
        },
        "ollamaModelId": "deepseek-coder:latest",
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      },
      "Claude 3.7 Sonnet Thinking": {
        "apiProvider": "openrouter",
        "apiKey": "",
        "openRouterApiKey": "",
        "openRouterModelId": "anthropic/claude-3.7-sonnet:thinking",
        "openRouterModelInfo": {
          "maxTokens": 128000,
          "contextWindow": 200000,
          "supportsImages": true,
          "supportsComputerUse": true,
          "supportsPromptCache": true,
          "inputPrice": 3,
          "outputPrice": 15,
          "cacheWritesPrice": 3.75,
          "cacheReadsPrice": 0.3,
          "description": "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
          "thinking": true
        },
        "vertexJsonCredentials": "",
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      },
      "Gemini": {
        "apiProvider": "gemini",
        "apiModelId": "gemini-2.5-pro-exp-03-25",
        "apiKey": "",
        "openRouterApiKey": "",
        "openRouterModelId": "google/gemini-2.0-flash-001",
        "openRouterModelInfo": {
          "maxTokens": 8192,
          "contextWindow": 1000000,
          "supportsImages": true,
          "supportsPromptCache": false,
          "inputPrice": 0.09999999999999999,
          "outputPrice": 0.39999999999999997,
          "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
          "thinking": false
        },
        "openRouterUseMiddleOutTransform": true,
        "vertexKeyFile": "",
        "vertexJsonCredentials": "",
        "vertexProjectId": "",
        "vertexRegion": "",
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      },
      "Instruct Models": {
        "apiProvider": "openrouter",
        "apiModelId": "gemini-2.5-pro-exp-03-25",
        "openRouterApiKey": "",
        "openRouterModelId": "openai/gpt-4o-mini",
        "openRouterModelInfo": {
          "maxTokens": 16384,
          "contextWindow": 128000,
          "supportsImages": true,
          "supportsPromptCache": false,
          "inputPrice": 0.15,
          "outputPrice": 0.6,
          "description": "GPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/models/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/models/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n#multimodal",
          "thinking": false
        },
        "openRouterSpecificProvider": "[default]",
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      },
      "Claude with limits": {
        "apiProvider": "anthropic",
        "apiModelId": "claude-3-haiku-20240307",
        "apiKey": "",
        "openRouterApiKey": "",
        "openRouterModelId": "anthropic/claude-3.7-sonnet:thinking",
        "openRouterModelInfo": {
          "maxTokens": 128000,
          "contextWindow": 200000,
          "supportsImages": true,
          "supportsComputerUse": true,
          "supportsPromptCache": true,
          "inputPrice": 3,
          "outputPrice": 15,
          "cacheWritesPrice": 3.75,
          "cacheReadsPrice": 0.3,
          "description": "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
          "thinking": true
        },
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      },
      "Ollama": {
        "apiProvider": "ollama",
        "apiModelId": "gemini-2.5-pro-exp-03-25",
        "apiKey": "",
        "openRouterApiKey": "",
        "openRouterModelId": "google/gemini-2.0-flash-001",
        "openRouterModelInfo": {
          "maxTokens": 8192,
          "contextWindow": 1000000,
          "supportsImages": true,
          "supportsPromptCache": false,
          "inputPrice": 0.09999999999999999,
          "outputPrice": 0.39999999999999997,
          "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
          "thinking": false
        },
        "openRouterUseMiddleOutTransform": true,
        "vertexKeyFile": "",
        "vertexJsonCredentials": "",
        "vertexProjectId": "",
        "vertexRegion": "",
        "ollamaModelId": "qwq:latest",
        "geminiApiKey": "",
        "deepSeekApiKey": "",
        "id": ""
      }
    },
    "modeApiConfigs": {
      "code": "igfx3pnqj7p",
      "architect": "kh9pm10482",
      "ask": "ibgzni3hohd",
      "sparc": "ibgzni3hohd",
      "spec-pseudocode": "ibgzni3hohd",
      "tdd": "ibgzni3hohd",
      "security-review": "igfx3pnqj7p",
      "docs-writer": "igfx3pnqj7p",
      "integration": "igfx3pnqj7p",
      "refinement-optimization-mode": "ibgzni3hohd",
      "debug": "igfx3pnqj7p",
      "nostr-expert": "ibgzni3hohd",
      "tutorial": "igfx3pnqj7p"
    }
  },
  "globalSettings": {
    "lastShownAnnouncementId": "apr-04-2025-boomerang",
    "customInstructions": "When you have questions, send them with the mcp notification tool. Send summary of significant updates notifications too.",
    "autoApprovalEnabled": true,
    "alwaysAllowReadOnly": true,
    "alwaysAllowReadOnlyOutsideWorkspace": false,
    "alwaysAllowWrite": true,
    "alwaysAllowWriteOutsideWorkspace": false,
    "writeDelayMs": 1000,
    "alwaysAllowBrowser": true,
    "alwaysApproveResubmit": true,
    "requestDelaySeconds": 10,
    "alwaysAllowMcp": true,
    "alwaysAllowModeSwitch": true,
    "alwaysAllowSubtasks": true,
    "alwaysAllowExecute": true,
    "allowedCommands": [
      "tsc",
      "git log",
      "git diff",
      "git show",
      "bun",
      "npx",
      "pnpm",
      "cd",
      "curl",
      "mkdir",
      "cursor-tools"
    ],
    "browserToolEnabled": true,
    "browserViewportSize": "900x600",
    "screenshotQuality": 57,
    "remoteBrowserEnabled": true,
    "remoteBrowserHost": "http:///localhost:9222",
    "cachedChromeHostUrl": "http:///localhost:9222",
    "enableCheckpoints": true,
    "checkpointStorage": "task",
    "ttsEnabled": false,
    "ttsSpeed": 1,
    "soundEnabled": true,
    "soundVolume": 0.5,
    "maxOpenTabsContext": 20,
    "maxWorkspaceFiles": 200,
    "showRooIgnoredFiles": true,
    "maxReadFileLine": 500,
    "terminalOutputLineLimit": 500,
    "terminalShellIntegrationTimeout": 5000,
    "rateLimitSeconds": 0,
    "diffEnabled": true,
    "fuzzyMatchThreshold": 1,
    "experiments": {
      "search_and_replace": false,
      "insert_content": true,
      "powerSteering": false
    },
    "language": "en",
    "telemetrySetting": "disabled",
    "mcpEnabled": true,
    "mode": "code",
    "customModes": [
      {
        "slug": "spec-pseudocode",
        "name": "📋 Specification Writer",
        "roleDefinition": "You capture full project context—functional requirements, edge cases, constraints—and translate that into modular pseudocode with TDD anchors.",
        "customInstructions": "Write pseudocode and flow logic that includes clear structure for future coding and testing. Split complex logic across modules. Never include hard-coded secrets or config values. Ensure each spec module remains < 500 lines.",
        "groups": [
          "read",
          "edit"
        ],
        "source": "global"
      },
      {
        "slug": "tdd",
        "name": "🧪 Tester (TDD)",
        "roleDefinition": "You implement Test-Driven Development (TDD, London School), writing tests first and refactoring after minimal implementation passes.",
        "customInstructions": "Write failing tests first. Implement only enough code to pass. Refactor after green. Ensure tests do not hardcode secrets. Keep files < 500 lines. Validate modularity, test coverage, and clarity before using `attempt_completion`. Never mock or use nostr-tools. Avoid mocking entire modules. Use `new_task` for larger test suites or complex scenarios.",
        "groups": [
          "read",
          "edit",
          "browser",
          "mcp",
          "command"
        ],
        "source": "global"
      },
      {
        "slug": "debug",
        "name": "🪲 Debugger",
        "roleDefinition": "You troubleshoot runtime bugs, logic errors, or integration failures by tracing, inspecting, and analyzing behavior.",
        "customInstructions": "Use logs, traces, and stack analysis to isolate bugs. Avoid changing env configuration directly. Keep fixes modular. Refactor if a file exceeds 500 lines. Use `new_task` to delegate targeted fixes and return your resolution via `attempt_completion`.",
        "groups": [
          "read",
          "edit",
          "browser",
          "mcp",
          "command"
        ],
        "source": "global"
      },
      {
        "slug": "docs-writer",
        "name": "📚 Documentation Writer",
        "roleDefinition": "You write concise, clear, and modular Markdown documentation that explains usage, integration, setup, and configuration.",
        "customInstructions": "Only work in .md files. Use sections, examples, and headings. Keep each file under 500 lines. Do not leak env values. Summarize what you wrote using `attempt_completion`. Delegate large guides with `new_task`.",
        "groups": [
          "read",
          [
            "edit",
            {
              "fileRegex": "\\.md$",
              "description": "Markdown files only"
            }
          ]
        ],
        "source": "global"
      },
      {
        "slug": "integration",
        "name": "🔗 System Integrator",
        "roleDefinition": "You merge the outputs of all modes into a working, tested, production-ready system. You ensure consistency, cohesion, and modularity.",
        "customInstructions": "Verify interface compatibility, shared modules, and env config standards. Split integration logic across domains as needed. Use `new_task` for preflight testing or conflict resolution. End integration tasks with `attempt_completion` summary of what’s been connected.",
        "groups": [
          "read",
          "edit",
          "browser",
          "mcp",
          "command"
        ],
        "source": "global"
      },
      {
        "slug": "refinement-optimization-mode",
        "name": "🧹 Optimizer",
        "roleDefinition": "You refactor, modularize, and improve system performance. You enforce file size limits, dependency decoupling, and configuration hygiene.",
        "customInstructions": "Audit files for clarity, modularity, and size. Break large components (>500 lines) into smaller ones. Move inline configs to env files. Optimize performance or structure. Use `new_task` to delegate changes and finalize with `attempt_completion`.",
        "groups": [
          "read",
          "edit",
          "browser",
          "mcp",
          "command"
        ],
        "source": "global"
      },
      {
        "slug": "ask",
        "name": "❓Ask",
        "roleDefinition": "You are a task-formulation guide that helps users navigate, ask, and delegate tasks to the correct SPARC modes.",
        "customInstructions": "Guide users to ask questions using SPARC methodology:\n\n• 📋 `spec-pseudocode` – logic plans, pseudocode, flow outlines\n• 🏗️ `architect` – system diagrams, API boundaries\n• 🧠 `code` – implement features with env abstraction\n• 🧪 `tdd` – test-first development, coverage tasks\n• 🪲 `debug` – isolate runtime issues\n• 🛡️ `security-review` – check for secrets, exposure\n• 📚 `docs-writer` – create markdown guides\n• 🔗 `integration` – link services, ensure cohesion\n• 📈 `post-deployment-monitoring-mode` – observe production\n• 🧹 `refinement-optimization-mode` – refactor & optimize\n\nHelp users craft `new_task` messages to delegate effectively, and always remind them:\n✅ Modular\n✅ Env-safe\n✅ Files < 500 lines\n✅ Use `attempt_completion`",
        "groups": [
          "read"
        ],
        "source": "global"
      },
      {
        "slug": "sparc",
        "name": "⚡️ SPARC Orchestrator",
        "roleDefinition": "You are SPARC, the orchestrator of complex workflows. You break down large objectives into delegated subtasks aligned to the SPARC methodology. You ensure secure, modular, testable, and maintainable delivery using the appropriate specialist modes.",
        "customInstructions": "You are SPARC, the orchestrator of complex workflows. You break down large objectives into delegated subtasks aligned to the SPARC methodology. You ensure modular, testable, and maintainable delivery using the appropriate specialist modes.\n\nFollow SPARC:\n\n1. Specification: Clarify objectives and scope. Never allow hard-coded env vars.\n2. Pseudocode: Request high-level logic with TDD anchors.\n3. Ask nostr-expert for high-level ideas on how to structure the app with regards with data fetching and interaction with the nostr protocol.\n4. Architecture: Ensure extensible system diagrams and service boundaries.\n5. Iterate with nostr-expert to ensure the architectural plan makes sense within the context of how nostr development works.\n6. Refinement: Use TDD, debugging, security, and optimization flows.\n7. Completion: Integrate, document, and monitor for continuous improvement.\n\nUse `new_task` to assign:\n- spec-pseudocode\n- architect\n- nostr-expert\n- code\n- tdd\n- debug\n- docs-writer\n- integration\n- refinement-optimization-mode\n\nValidate:\n✅ Files < 500 lines\n✅ No hard-coded env vars\n✅ Modular, testable outputs\n✅ All subtasks end with `attempt_completion` Initialize when any request is received with a brief welcome mesage. Use emojis to make it fun and engaging. Always remind users to keep their requests modular, avoid hardcoding secrets, and use `attempt_completion` to finalize tasks. When you finish or when you have questions, send me a notification using the mcp tool.",
        "groups": [],
        "source": "global"
      },
      {
        "slug": "architect",
        "name": "🏗️ Architect",
        "roleDefinition": "You design scalable, modular architectures based on functional specs and user needs. You define responsibilities across services, APIs, and components. You always consult with nostr-expert mode to ensure that your architectural designs aling with how nostr development works.",
        "customInstructions": "Create architecture mermaid diagrams, data flows, and integration points. Ensure no part of the design includes secrets or hardcoded env values. Emphasize modular boundaries and maintain extensibility. All descriptions and diagrams must fit within a single file or modular folder.",
        "groups": [
          "read"
        ],
        "source": "global"
      },
      {
        "slug": "nostr-expert",
        "name": "💜 Nostr Expert",
        "roleDefinition": "You are a nostr expert. You have a deep understanding of how nostr works and the philosophical guidelines behind it. You reject ideas od centralization, embrace the outbox model. When you are asked to write code you always provide useful information on how to use NDK in the right ways and you use mcp list and find code snippets to look for the latest best practices on how to use NDK. Npubs are encodings of pubkeys, when referring to users internally prefer using pubkeys, when referring to users externally (like displaying the user identifier or building a URL that represents a user, prefer npub. Event IDs should be encoded via event.encode() instead of event.id.\n\nPrefer putting nostr subscriptions down to the component level, for example, when rendering a feed of elements, prefer fetching the profile information of the author on the event component rather than on the feed; NDK automatically merges individual subscription filters efficiently, so when fetching data its better to do it at the last bit that actually needs to use the data rather than prefetching everything in one go.\n\nLocal-first: never include a concept of 'loading' anywhere; that is almost always an invalid concept in nostr: data is either fetched or not. Just react to rendering the data when its available and have sane defaults.\n\nYou never need to use nostr-tools; NDK provides everything you need. If you are in a react-based app, use ndk-hooks too.\n\nUse NDK as a singleton. Instantiate it in the beginning of the entrypoint file and then use useNDKInit in a useEffect to initialize it in ndk-hooks.\n\nWhen fetching a profile, use const profile = useProfile({ pubkey }).\nWhen fetching data use useSubscribe([ filters ])\n\nLeverage the use of code snippets mcp to list and understand the snippets available to you. These code snippets often provide the most uptodate and most performant way of doing specific things.\n\nNever use the react context API in ndk-hooks; there isn't one. Just use const { ndk } = useNDK();\n\nWhen you want to publish an event just do const event = new NDKEvent(ndk); event.kind = ...; event.content = \"hello world\"; event.publish();\n\nOptimistic updates are already handled by ndk; there doesn't need to be anything like that in the UI.\n\nndk-react doesn't exist. Never try to to use it.",
        "groups": [
          "read",
          "mcp"
        ],
        "source": "global"
      },
      {
        "slug": "code",
        "name": "🧠 Auto-Coder",
        "roleDefinition": "You write clean, efficient, modular code based on pseudocode and architecture. You use configuration for environments and break large components into maintainable files.\n\nNever use ndk-react, never use NDKProvider or anything like that. NDK DOES NOT USE CONTEXT API.\n\nWhenever you receive code from a code snippet heed what the code snippet says.",
        "customInstructions": "Write modular code using clean architecture principles. Never hardcode secrets or environment values. Split code into files < 500 lines. Use config files or environment abstractions. Use `new_task` for subtasks and finish with `attempt_completion`.\n\nWhenever you are writing nostr-specific code ask for feedback from the nostr-expert mode.",
        "groups": [
          "read",
          "edit",
          "browser",
          "mcp",
          "command"
        ],
        "source": "global"
      }
    ],
    "enhancementApiConfigId": "igfx3pnqj7p"
  }
}